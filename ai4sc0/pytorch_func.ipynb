{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch的函数变换\n",
    "\n",
    "\n",
    "下面介绍一下由torch.func提供的函数变换功能，这在使用PyTorch做科学计算时尤为有用。我们下面展示一些它的例子。\n",
    "\n",
    "## 自动向量化并行\n",
    "\n",
    "下面我们定义一个函数，这个函数的作用是对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:33:50.382326Z",
     "start_time": "2023-04-14T07:33:50.377880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [2., 4., 6.],\n",
      "        [3., 6., 9.]])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 2.,  4.,  6.],\n",
      "         [ 3.,  6.,  9.]],\n",
      "\n",
      "        [[ 4.,  8., 12.],\n",
      "         [ 8., 16., 24.],\n",
      "         [12., 24., 36.]],\n",
      "\n",
      "        [[ 9., 18., 27.],\n",
      "         [18., 36., 54.],\n",
      "         [27., 54., 81.]]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [2., 4., 6.],\n",
      "        [3., 6., 9.]])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 2.,  4.,  6.],\n",
      "         [ 3.,  6.,  9.]],\n",
      "\n",
      "        [[ 4.,  8., 12.],\n",
      "         [ 8., 16., 24.],\n",
      "         [12., 24., 36.]],\n",
      "\n",
      "        [[ 9., 18., 27.],\n",
      "         [18., 36., 54.],\n",
      "         [27., 54., 81.]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import vmap\n",
    "\n",
    "def f(x):\n",
    "    return torch.outer(x,x)\n",
    "\n",
    "x=torch.arange(1.,4.)\n",
    "print(f(x))\n",
    "bx=vmap(f)(f(x))\n",
    "print(bx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个例子可能过于简单，显示不出来这个函数的威力。但是对于PyTorch写成的几乎所有函数，都可以这么做，包括复杂的求解器，例如我们写的RTE求解器。例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:33:51.124633Z",
     "start_time": "2023-04-14T07:33:50.384656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 50, 400]) torch.Size([4, 50, 400]) torch.Size([4, 1, 400])\n",
      "torch.Size([4, 50, 400]) torch.Size([4, 50, 400]) torch.Size([4, 1, 400])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def get_gauss_point(n):\n",
    "    x, w = np.polynomial.legendre.leggauss(n)\n",
    "    return torch.from_numpy(x).to(dtype=torch.get_default_dtype()), torch.from_numpy(w).to(dtype=torch.get_default_dtype())\n",
    "\n",
    "a = 1\n",
    "c = 1\n",
    "rho = 1\n",
    "cv = 1\n",
    "\n",
    "# spatial grid\n",
    "N = 400\n",
    "L = 1\n",
    "dx = L / N\n",
    "x = torch.arange(0.5 * dx, L + 0.5 * dx, dx)\n",
    "\n",
    "Tini =  vmap(lambda cas:(1 + 0.1 * torch.sin(2 * math.pi * x+torch.sin(cas)))*(1+torch.sin(cas)))(torch.tensor([1,2,3,4]))\n",
    "sigma = vmap(lambda cas:(1 + 0.1 * torch.sin(2 * math.pi * x+torch.sin(cas+1)))*(1+torch.sin(cas)))(torch.tensor([1,2,3,4]))\n",
    "    \n",
    "def solve(Tini,sigma):\n",
    "    CFL = 0.8\n",
    "    dt = CFL * dx  # time step\n",
    "    dtc = dt * c\n",
    "    ddtc = 1 / dtc\n",
    "\n",
    "    datarecord_sigma=sigma.clone()[None,:]\n",
    "    \n",
    "    # velocity grid & angle\n",
    "    Nvx = 8\n",
    "    mu, wmu = get_gauss_point(Nvx)\n",
    "\n",
    "    # distribution function\n",
    "    T = Tini\n",
    "    I = 0.5 * a * c * Tini**4\n",
    "    I = I.repeat(Nvx, 1)\n",
    "    I = F.pad(I[None, ...], (1, 1), mode='circular')[0]\n",
    "    I0 = wmu @ I  # energe\n",
    "    sigma = sigma.repeat(Nvx // 2, 1)\n",
    "\n",
    "    t=1.0\n",
    "    Nt=int(t / dt)\n",
    "    list_T=[]\n",
    "    list_E=[]\n",
    "    for loop in range(Nt):  #=1: 1/dt\n",
    "        I_out = I.clone()\n",
    "        T_out = T.clone()\n",
    "        I0_out = I0.clone()\n",
    "\n",
    "        index = slice(1, -1)\n",
    "        index_add1 = slice(2, None)\n",
    "        index_sub1 = slice(None, -2)\n",
    "\n",
    "        # streaming, positive vx\n",
    "        lv = slice(Nvx // 2, None)\n",
    "        coe = mu[lv]\n",
    "        I[lv, index] = I_out[lv, index] - dt / dx * coe[..., None] * (\n",
    "            I_out[lv, index] - I_out[lv, index_sub1]) + dt * sigma * (\n",
    "                (0.5 * a * c * T_out**4).repeat(Nvx // 2, 1) - I_out[lv, index])\n",
    "        \n",
    "        # streaming, negative vx\n",
    "        lv = slice(0, Nvx // 2)\n",
    "        coe = mu[lv]\n",
    "        I[lv, index] = I_out[lv, index] - dt / dx * coe[..., None] * (\n",
    "            I_out[lv, index_add1] - I_out[lv, index]) + dt * sigma * (\n",
    "                (0.5 * a * c * T_out**4).repeat(Nvx // 2, 1) - I_out[lv, index])\n",
    "\n",
    "        I = F.pad(I[None, ..., 1:-1], (1, 1), mode='circular')[0]\n",
    "        T = T_out + dt / cv * sigma[0, :] * (I0_out[index] - a * c * T_out**4)\n",
    "        I0 = wmu @ I\n",
    "\n",
    "        if loop%10==0:\n",
    "            list_E.append(I0[...,1:-1].clone())\n",
    "            list_T.append(T.clone())\n",
    "    datarecord_E=torch.stack(list_E,dim=0)\n",
    "    datarecord_T=torch.stack(list_T,dim=0)\n",
    "    return datarecord_E,datarecord_T,datarecord_sigma\n",
    "\n",
    "E,T,sigma=vmap(solve)(Tini,sigma)\n",
    "print(E.shape,T.shape,sigma.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导\n",
    "\n",
    "最简单的就是使用grad求函数导数，它假设函数返回的是一个值，然后求这个值对于输入的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:33:51.132848Z",
     "start_time": "2023-04-14T07:33:51.126517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9999), tensor(0.9999))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.9999), tensor(0.9999))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import grad\n",
    "x = torch.randn([]) #可以试试改形状会发生什么\n",
    "fx = lambda x: torch.sin(x)\n",
    "cos_x = grad(fx)(x)\n",
    "\n",
    "torch.cos(x),cos_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果输出的不是一个值，那么grad方法就不再起作用了。这时，根据我们想要的导数不同，有多种不同的做法。例如对于sin这个例子，假设输入的是一个向量，我们其实只想求输出的每个值对应输入的每个值的导，就可以用vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:33:51.150944Z",
     "start_time": "2023-04-14T07:33:51.134537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3396,  0.8663,  0.8830,  0.7084,  0.9351, -0.1285, -0.0045,  0.8071,\n",
       "         -0.1174,  0.9670]),\n",
       " tensor([ 0.3396,  0.8663,  0.8830,  0.7084,  0.9351, -0.1285, -0.0045,  0.8071,\n",
       "         -0.1174,  0.9670]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.3396,  0.8663,  0.8830,  0.7084,  0.9351, -0.1285, -0.0045,  0.8071,\n",
       "         -0.1174,  0.9670]),\n",
       " tensor([ 0.3396,  0.8663,  0.8830,  0.7084,  0.9351, -0.1285, -0.0045,  0.8071,\n",
       "         -0.1174,  0.9670]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn([10,]) #可以试试改形状会发生什么\n",
    "fx=lambda x: torch.sin(x)\n",
    "cos_x = vmap(grad(fx))(x) #grad函数被向量化到了一个vector上\n",
    "\n",
    "torch.cos(x),cos_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种情况，就是我们想要求的就是输出的每个值对输入的每个值的导数，此时我们相求的是jacobian矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:33:51.165143Z",
     "start_time": "2023-04-14T07:33:51.153503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.9992,  0.7487,  0.9434,  0.9032,  0.9164,  0.6165,  0.5111,  1.0000,\n",
       "          0.7079, -0.1347]),\n",
       " tensor([[ 0.9992,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.7487,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.9434,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.9032,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9164,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6165,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5111,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.7079, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.1347]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) torch.Size([10]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.9992,  0.7487,  0.9434,  0.9032,  0.9164,  0.6165,  0.5111,  1.0000,\n",
       "          0.7079, -0.1347]),\n",
       " tensor([[ 0.9992,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.7487,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.9434,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.9032,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.9164,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.6165,  0.0000,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.5111,  0.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000,\n",
       "           0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.7079, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000, -0.1347]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.func import jacrev,jacfwd,hessian\n",
    "\n",
    "x = torch.randn([10,]) #可以试试改形状会发生什么\n",
    "fx=lambda x: torch.sin(x)\n",
    "cos_x = jacrev(fx)(x) #grad函数被向量化到了一个vector上\n",
    "\n",
    "print(cos_x.shape,x.shape,fx(x).shape)\n",
    "torch.cos(x),cos_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:33:51.199268Z",
     "start_time": "2023-04-14T07:33:51.167656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 10, 2, 10, 2]) torch.Size([10, 2])\n",
      "torch.Size([10, 2, 10, 2, 10, 2]) torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "from torch.func import jacrev,jacfwd,hessian\n",
    "\n",
    "x = torch.randn([10,2]) #可以试试改形状会发生什么\n",
    "fx=lambda x: torch.sin(x)\n",
    "hessian_x = hessian(fx)(x) #grad函数被向量化到了一个vector上\n",
    "\n",
    "print(hessian_x.shape,x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更为常见的情况是以上两种的混合，例如，我们用一个神经网络去拟合二维的Euler方程的解，这个网络是从向量到向量的一个映射，我们希望计算得到$\\frac{\\partial f}{\\partial (x,t)}$这样一个有6个元素的jacobian。同时我们网络的输入是batchsize=1000，我们就要同时计算这1000个sample的jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-14T07:33:51.262010Z",
     "start_time": "2023-04-14T07:33:51.201449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([1000, 2]),y.shape=torch.Size([1000, 3]),y2.shape=torch.Size([1000, 3])\n",
      "dy_dx_1.shape=torch.Size([1000, 3, 2]),dy_dx_2.shape=torch.Size([1000, 3, 2])\n",
      "ddy_dxx.shape=torch.Size([1000, 3, 2, 2]),ddy_dxx2.shape=torch.Size([1000, 3, 2, 2]),ddy_dxx3.shape=torch.Size([1000, 3, 2, 2])\n",
      "x.shape=torch.Size([1000, 2]),y.shape=torch.Size([1000, 3]),y2.shape=torch.Size([1000, 3])\n",
      "dy_dx_1.shape=torch.Size([1000, 3, 2]),dy_dx_2.shape=torch.Size([1000, 3, 2])\n",
      "ddy_dxx.shape=torch.Size([1000, 3, 2, 2]),ddy_dxx2.shape=torch.Size([1000, 3, 2, 2]),ddy_dxx3.shape=torch.Size([1000, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([1000,2])\n",
    "Net=torch.nn.Linear(2,3)\n",
    "y=Net(x)\n",
    "y2=vmap(Net)(x)\n",
    "print(f\"{x.shape=},{y.shape=},{y2.shape=}\")\n",
    "\n",
    "dy_dx_1=vmap(jacrev(Net))(x)\n",
    "dy_dx_2=vmap(jacfwd(Net))(x)\n",
    "print(f\"{dy_dx_1.shape=},{dy_dx_2.shape=}\")\n",
    "\n",
    "def myhessian(f):\n",
    "    return jacfwd(jacrev(f))\n",
    "def myhessian2(f):\n",
    "    return jacrev(jacfwd(f))\n",
    "\n",
    "ddy_dxx=vmap(hessian(Net))(x)\n",
    "ddy_dxx2=vmap(myhessian(Net))(x)\n",
    "ddy_dxx3=vmap(myhessian2(Net))(x)\n",
    "\n",
    "print(f\"{ddy_dxx.shape=},{ddy_dxx2.shape=},{ddy_dxx3.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何选择jacrev与jacfwd\n",
    "\n",
    "\n",
    "jacrev与jacfwd\n",
    "> These two functions compute the same values (up to machine numerics), but differ in their implementation: jacfwd uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while jacrev uses reverse-mode, which is more efficient for “wide” Jacobian matrices. For matrices that are near-square, jacfwd probably has an edge over jacrev.\n",
    "\n",
    "以及对于hessian\n",
    "\n",
    "> To implement hessian, we could have used jacfwd(jacrev(f)) or jacrev(jacfwd(f)) or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function 𝑓:ℝⁿ→ℝ), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since ∇𝑓:ℝⁿ→ℝⁿ), which is where forward-mode wins out.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
